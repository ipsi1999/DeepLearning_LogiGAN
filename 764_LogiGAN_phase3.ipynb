{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":33148,"status":"ok","timestamp":1761184542083,"user":{"displayName":"Mohor Bhattacharya","userId":"06292635116005505970"},"user_tz":-780},"id":"E9kkeAPzahHD"},"outputs":[],"source":["\"\"\"\n","INTEGRATED: EXPERIMENT 2 → EXPERIMENT 3 PIPELINE\n","================================================\n","\n","This script runs:\n","1. EXPERIMENT 2: Train adversarial classifier with hard negative mining\n","2. EXPERIMENT 3: Apply contrastive learning on logical pairs using the trained model\n","\n","Your friend can run this entire pipeline in one go!\n","\n","Usage:\n","    python integrated_exp2_exp3.py\n","\n","Estimated time: 3-4 hours on CPU, 1-1.5 hours on GPU\n","\"\"\"\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import random\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import (\n","    DistilBertModel,\n","    DistilBertTokenizer,\n","    DistilBertForSequenceClassification,\n","    get_linear_schedule_with_warmup\n",")\n","from torch.optim import AdamW\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","import re\n","import json\n","from collections import defaultdict\n","from pathlib import Path\n","# Set random seeds\n","def set_seed(seed=42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","\n","set_seed(42)"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1761184542096,"user":{"displayName":"Mohor Bhattacharya","userId":"06292635116005505970"},"user_tz":-780},"id":"in0bi7eYa06o"},"outputs":[],"source":["# ============================================================================\n","# EXPERIMENT 2 COMPONENTS (From your notebook)\n","# ============================================================================\n","\n","class LogicalSentenceDetector:\n","    \"\"\"Enhanced heuristic-based detector for logical sentences\"\"\"\n","\n","    def __init__(self):\n","        self.conditional_words = [\n","            'if', 'then', 'when', 'whenever', 'unless',\n","            'provided that', 'as long as', 'in case', 'supposing',\n","            'assuming', 'given that', 'on condition that'\n","        ]\n","        self.causal_words = [\n","            'because', 'since', 'as', 'due to', 'owing to',\n","            'therefore', 'thus', 'hence', 'consequently', 'so',\n","            'for this reason', 'as a result', 'leads to', 'causes'\n","        ]\n","        self.contrast_words = [\n","            'although', 'though', 'however', 'but', 'yet',\n","            'nevertheless', 'nonetheless', 'despite', 'in spite of',\n","            'whereas', 'while', 'on the other hand', 'conversely'\n","        ]\n","        self.conjunction_words = ['and', 'or', 'nor', 'either', 'neither']\n","\n","        self.all_logic_words = (\n","            self.conditional_words + self.causal_words +\n","            self.contrast_words + self.conjunction_words\n","        )\n","\n","    def compute_logic_score(self, sentence):\n","        \"\"\"Compute a confidence score for logical structure (0-1)\"\"\"\n","        sentence_lower = sentence.lower()\n","        score = 0.0\n","\n","        connective_count = sum(1 for word in self.all_logic_words if word in sentence_lower)\n","        score += min(0.4, connective_count * 0.2)\n","\n","        if ',' in sentence or ';' in sentence:\n","            score += 0.3\n","\n","        clause_count = sentence.count(',') + sentence.count(';')\n","        score += min(0.2, clause_count * 0.1)\n","\n","        if re.search(r'if .+, .+', sentence_lower):\n","            score += 0.1\n","        elif re.search(r'because .+, .+', sentence_lower):\n","            score += 0.1\n","\n","        return min(1.0, score)\n","\n","    def is_logical(self, sentence, threshold=0.5):\n","        return self.compute_logic_score(sentence) >= threshold\n","\n","    def get_logic_type(self, sentence):\n","        sentence_lower = sentence.lower()\n","\n","        if any(word in sentence_lower for word in self.conditional_words):\n","            return 'conditional'\n","        elif any(word in sentence_lower for word in self.causal_words):\n","            return 'causal'\n","        elif any(word in sentence_lower for word in self.contrast_words):\n","            return 'contrast'\n","        elif any(word in sentence_lower for word in self.conjunction_words):\n","            return 'conjunction'\n","        return 'none'"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1761184542098,"user":{"displayName":"Mohor Bhattacharya","userId":"06292635116005505970"},"user_tz":-780},"id":"Mb4Gr6rba1L1"},"outputs":[],"source":["class AdversarialGenerator:\n","    \"\"\"Generate adversarial examples through multiple perturbation strategies\"\"\"\n","\n","    def __init__(self):\n","        self.negations = ['not', 'never', \"n't\"]\n","\n","        self.connective_synonyms = {\n","            'if': ['when', 'whenever', 'in case', 'supposing'],\n","            'because': ['since', 'as', 'due to the fact that'],\n","            'although': ['though', 'even though', 'despite the fact that'],\n","            'therefore': ['thus', 'hence', 'consequently', 'as a result'],\n","            'however': ['nevertheless', 'nonetheless', 'yet', 'but'],\n","            'when': ['whenever', 'as', 'while'],\n","        }\n","\n","        self.opposing_connectives = {\n","            'because': ['although', 'despite', 'even though'],\n","            'although': ['because', 'since'],\n","            'if': ['even if', 'unless'],\n","            'therefore': ['however', 'nevertheless'],\n","            'and': ['but', 'yet'],\n","        }\n","\n","        self.verb_synonyms = {\n","            'studied': ['learned', 'reviewed', 'practiced'],\n","            'passed': ['succeeded', 'completed', 'aced'],\n","            'rains': ['pours', 'drizzles', 'showers'],\n","            'rises': ['comes up', 'appears', 'ascends'],\n","            'falls': ['drops', 'decreases', 'declines'],\n","            'grows': ['develops', 'expands', 'increases'],\n","        }\n","\n","    def add_negation(self, sentence):\n","        verbs = ['is', 'are', 'was', 'were', 'will', 'would', 'can', 'could', 'should', 'may', 'might']\n","        for verb in verbs:\n","            pattern = rf'\\b{verb}\\b\\s+'\n","            if re.search(pattern, sentence, re.IGNORECASE):\n","                return re.sub(pattern, f'{verb} not ', sentence, count=1, flags=re.IGNORECASE)\n","\n","        if ',' in sentence:\n","            parts = sentence.split(',', 1)\n","            if len(parts) == 2:\n","                return parts[0] + ', not' + parts[1]\n","        return sentence\n","\n","    def remove_negation(self, sentence):\n","        sentence = re.sub(r'\\bnot\\b\\s*', '', sentence, flags=re.IGNORECASE)\n","        sentence = re.sub(r'\\bnever\\b', 'always', sentence, flags=re.IGNORECASE)\n","        sentence = re.sub(r\"n't\\b\", '', sentence)\n","        return sentence\n","\n","    def flip_connective(self, sentence):\n","        sentence_lower = sentence.lower()\n","\n","        for original, opposites in self.opposing_connectives.items():\n","            if original in sentence_lower:\n","                replacement = random.choice(opposites)\n","                return re.sub(rf'\\b{original}\\b', replacement, sentence, count=1, flags=re.IGNORECASE)\n","\n","        return sentence\n","\n","    def remove_connective(self, sentence):\n","        connectives = [\n","            'if', 'because', 'since', 'although', 'when', 'though',\n","            'therefore', 'thus', 'hence', 'so', 'but', 'however'\n","        ]\n","        for conn in connectives:\n","            pattern = rf'\\b{conn}\\b[,\\s]*'\n","            result = re.sub(pattern, '', sentence, flags=re.IGNORECASE, count=1)\n","            if result != sentence:\n","                return result\n","        return sentence\n","\n","    def swap_clauses(self, sentence):\n","        if ',' in sentence:\n","            parts = sentence.split(',', 1)\n","            if len(parts) == 2:\n","                clause1 = parts[0].strip()\n","                clause2 = parts[1].strip()\n","                clause2 = clause2[0].upper() + clause2[1:] if clause2 else clause2\n","                clause1 = clause1[0].lower() + clause1[1:] if clause1 else clause1\n","                return f\"{clause2}, {clause1}\"\n","        return sentence\n","\n","    def synonym_replacement(self, sentence):\n","        sentence_lower = sentence.lower()\n","\n","        for original, synonyms in self.connective_synonyms.items():\n","            if original in sentence_lower:\n","                replacement = random.choice(synonyms)\n","                return re.sub(rf'\\b{original}\\b', replacement, sentence, count=1, flags=re.IGNORECASE)\n","\n","        return sentence\n","\n","    def paraphrase_clause(self, sentence):\n","        for original, synonyms in self.verb_synonyms.items():\n","            if original in sentence.lower():\n","                replacement = random.choice(synonyms)\n","                sentence = re.sub(rf'\\b{original}\\b', replacement, sentence, count=1, flags=re.IGNORECASE)\n","                break\n","        return sentence\n","\n","    def weaken_logic(self, sentence):\n","        hedges = ['probably', 'possibly', 'might', 'perhaps', 'usually']\n","        hedge = random.choice(hedges)\n","\n","        verbs = ['is', 'are', 'will', 'would', 'can']\n","        for verb in verbs:\n","            pattern = rf'\\b{verb}\\b\\s+'\n","            if re.search(pattern, sentence, re.IGNORECASE):\n","                return re.sub(pattern, f'{verb} {hedge} ', sentence, count=1, flags=re.IGNORECASE)\n","\n","        return sentence\n","\n","    def generate_perturbation(self, sentence, perturbation_type='random'):\n","        if perturbation_type == 'random':\n","            perturbation_type = random.choice([\n","                'add_negation', 'remove_negation', 'flip_connective',\n","                'remove_connective', 'swap_clauses', 'synonym_replacement',\n","                'paraphrase_clause', 'weaken_logic'\n","            ])\n","\n","        try:\n","            if perturbation_type == 'add_negation':\n","                return self.add_negation(sentence)\n","            elif perturbation_type == 'remove_negation':\n","                return self.remove_negation(sentence)\n","            elif perturbation_type == 'flip_connective':\n","                return self.flip_connective(sentence)\n","            elif perturbation_type == 'remove_connective':\n","                return self.remove_connective(sentence)\n","            elif perturbation_type == 'swap_clauses':\n","                return self.swap_clauses(sentence)\n","            elif perturbation_type == 'synonym_replacement':\n","                return self.synonym_replacement(sentence)\n","            elif perturbation_type == 'paraphrase_clause':\n","                return self.paraphrase_clause(sentence)\n","            elif perturbation_type == 'weaken_logic':\n","                return self.weaken_logic(sentence)\n","        except:\n","            return sentence\n","\n","        return sentence\n","\n","    def generate_multi_perturbation(self, sentence, num_perturbations=2):\n","        result = sentence\n","        perturbation_types = [\n","            'add_negation', 'flip_connective', 'remove_connective',\n","            'swap_clauses', 'weaken_logic'\n","        ]\n","\n","        selected_types = random.sample(perturbation_types, min(num_perturbations, len(perturbation_types)))\n","\n","        for ptype in selected_types:\n","            result = self.generate_perturbation(result, ptype)\n","            if result == sentence:\n","                result = self.generate_perturbation(result, 'random')\n","\n","        return result"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1761184542123,"user":{"displayName":"Mohor Bhattacharya","userId":"06292635116005505970"},"user_tz":-780},"id":"GB7i7mBPa1PO"},"outputs":[],"source":["def create_expanded_dataset():\n","    \"\"\"Create comprehensive dataset with 300+ sentences (from your notebook)\"\"\"\n","\n","    logical_sentences = [\n","        # Your exact sentences from the notebook\n","        \"If it rains, the ground will be wet.\",\n","        \"If you heat water to 100°C, it boils.\",\n","        \"If the battery dies, the phone won't work.\",\n","        \"If you save money, you can buy what you want.\",\n","        \"When the sun sets, the temperature drops.\",\n","        \"When plants get sunlight, they grow faster.\",\n","        \"When you exercise regularly, your health improves.\",\n","        \"When the alarm rings, people evacuate the building.\",\n","        \"When prices rise, demand typically falls.\",\n","        \"When ice melts, it becomes water.\",\n","        \"If you touch fire, you get burned.\",\n","        \"If the door is locked, you cannot enter.\",\n","        \"If she calls, please let me know.\",\n","        \"When winter comes, birds migrate south.\",\n","        \"When the light turns red, cars must stop.\",\n","        \"If you don't water plants, they will die.\",\n","        \"When the temperature drops below zero, water freezes.\",\n","        \"If you study hard, you'll likely succeed.\",\n","        \"When the movie ends, the lights come on.\",\n","        \"If the wifi is down, we can't work online.\",\n","        \"When you mix blue and yellow, you get green.\",\n","        \"If the train is delayed, we'll miss our connection.\",\n","        \"When the battery is full, the charging stops.\",\n","        \"If you break the rules, there are consequences.\",\n","        \"When the season changes, fashion trends shift.\",\n","        \"If the economy grows, employment increases.\",\n","        \"When the sun shines, solar panels generate power.\",\n","        \"If you forget the password, you can't log in.\",\n","        \"When the timer beeps, the food is ready.\",\n","        \"If the signal is weak, calls drop frequently.\",\n","        \"Because he studied hard, he passed the exam.\",\n","        \"Since the store was closed, we went home.\",\n","        \"Because the road was icy, traffic moved slowly.\",\n","        \"Since he missed the bus, he arrived late.\",\n","        \"Because the bridge collapsed, the road was closed.\",\n","        \"Because the weather was nice, we went to the park.\",\n","        \"Since the evidence was clear, the jury convicted him.\",\n","        \"Because she was sick, she stayed home from work.\",\n","        \"Since the project was urgent, they worked overtime.\",\n","        \"Because prices increased, sales declined.\",\n","        \"Since the restaurant was full, we waited outside.\",\n","        \"Because the water was contaminated, people got sick.\",\n","        \"Since he forgot his keys, he couldn't enter.\",\n","        \"Because the storm was severe, flights were cancelled.\",\n","        \"Since the deadline passed, submissions were closed.\",\n","        \"Because the team practiced daily, they won the championship.\",\n","        \"Since the equipment broke, production stopped.\",\n","        \"Because the film was popular, tickets sold out quickly.\",\n","        \"Since she had experience, she got the job.\",\n","        \"Because the road was blocked, we took a detour.\",\n","        \"Therefore, we must act quickly to solve this problem.\",\n","        \"Thus, the hypothesis was proven correct.\",\n","        \"Hence, the company decided to expand operations.\",\n","        \"Consequently, many people lost their jobs.\",\n","        \"As a result, the ecosystem was severely damaged.\",\n","        \"Therefore, further research is necessary.\",\n","        \"Thus, the treaty was signed by all parties.\",\n","        \"Hence, the policy was changed immediately.\",\n","        \"Consequently, sales increased by thirty percent.\",\n","        \"As a result, the building had to be demolished.\",\n","        \"Although she was tired, she finished the project.\",\n","        \"Although it was expensive, they bought the car.\",\n","        \"Although the task was difficult, she succeeded.\",\n","        \"Despite the rain, the game continued.\",\n","        \"Though he was young, he was very wise.\",\n","        \"Although they lost, they played their best.\",\n","        \"Despite the warning, he proceeded anyway.\",\n","        \"Though it was late, they kept working.\",\n","        \"Although the evidence was weak, they prosecuted.\",\n","        \"Despite the risks, she took the job.\",\n","        \"Although the road was long, they kept walking.\",\n","        \"Though the odds were against them, they won.\",\n","        \"Despite the cost, quality is worth it.\",\n","        \"Although it was crowded, we found seats.\",\n","        \"Though he apologized, she remained angry.\",\n","        \"Despite the delay, we arrived on time.\",\n","        \"Although the recipe was complex, she made it perfectly.\",\n","        \"Though the exam was hard, most students passed.\",\n","        \"Despite the competition, our product succeeded.\",\n","        \"Although he was injured, he finished the race.\",\n","        \"However, the results were not what we expected.\",\n","        \"Nevertheless, the plan moved forward.\",\n","        \"The weather was bad, but the event proceeded anyway.\",\n","        \"She was exhausted, yet she continued working.\",\n","        \"The task was daunting, however they persevered.\",\n","        \"It was expensive, but the quality justified the price.\",\n","        \"He was nervous, yet he delivered an excellent speech.\",\n","        \"The journey was long, nevertheless they enjoyed it.\",\n","        \"The situation was dire, but hope remained.\",\n","        \"The odds were slim, yet they took the chance.\",\n","        \"If you water the plants and give them sunlight, they will flourish.\",\n","        \"Because the temperature dropped and the roads were icy, schools closed.\",\n","        \"Although she studied hard and prepared well, she was still nervous.\",\n","        \"When the alarm sounds and smoke is detected, evacuate immediately.\",\n","        \"If you combine effort with strategy, success becomes likely.\",\n","        \"Since the data was analyzed and patterns emerged, conclusions were drawn.\",\n","        \"Although the plan was risky and resources were limited, they proceeded.\",\n","        \"When technology advances and costs decrease, adoption increases.\",\n","        \"If ingredients are fresh and preparation is careful, meals taste better.\",\n","        \"Because demand was high and supply was low, prices soared.\",\n","        \"When citizens vote and participate actively, democracy thrives.\",\n","        \"If you listen carefully and ask questions, you learn more.\",\n","        \"Since the evidence was overwhelming and witnesses testified, the verdict was guilty.\",\n","        \"Although the journey was difficult and setbacks occurred, they reached their goal.\",\n","        \"When interest rates fall and borrowing becomes cheaper, economies grow.\",\n","        \"If systems are tested and bugs are fixed, software becomes reliable.\",\n","        \"Because training was thorough and equipment was modern, performance improved.\",\n","        \"When communication is clear and expectations are set, teams succeed.\",\n","        \"If you save consistently and invest wisely, wealth accumulates.\",\n","        \"Since regulations were strict and enforcement was strong, compliance improved.\",\n","        \"Unless you hurry, you will miss the train.\",\n","        \"Provided that you complete the assignment, you will pass.\",\n","        \"As long as you follow the rules, there won't be problems.\",\n","        \"In case of emergency, break the glass.\",\n","        \"Whenever she visits, she brings gifts.\",\n","        \"Since morning, the situation has deteriorated.\",\n","        \"Given that the facts support it, we should proceed.\",\n","        \"Assuming the weather holds, the picnic is on.\",\n","        \"On condition that you agree, we can move forward.\",\n","        \"Whereas some prefer coffee, others choose tea.\",\n","        \"While technology helps productivity, it also creates distractions.\",\n","        \"Either you adapt to change, or you get left behind.\",\n","        \"Neither rain nor snow will stop the delivery.\",\n","        \"Not only did she win, but she also set a record.\",\n","        \"Both the theory and the evidence support this conclusion.\",\n","        \"Just as the sun rises in the east, it sets in the west.\",\n","        \"The more you practice, the better you become.\",\n","        \"The harder you work, the luckier you get.\",\n","        \"No sooner had she left than the phone rang.\",\n","        \"Hardly had the game begun when it started raining.\",\n","    ]\n","\n","    non_logical_sentences = [\n","        # Your exact sentences from the notebook\n","        \"The cat sleeps on the couch.\",\n","        \"She loves chocolate ice cream.\",\n","        \"The building is very tall.\",\n","        \"He drives a blue car.\",\n","        \"The movie was entertaining.\",\n","        \"They live in a small town.\",\n","        \"The coffee tastes bitter.\",\n","        \"She has three siblings.\",\n","        \"The book is on the table.\",\n","        \"He plays guitar every day.\",\n","        \"The flowers smell wonderful.\",\n","        \"She graduated last year.\",\n","        \"The museum opens at nine.\",\n","        \"He enjoys reading novels.\",\n","        \"The stars are bright tonight.\",\n","        \"She works as a teacher.\",\n","        \"The pizza was delicious.\",\n","        \"He speaks three languages.\",\n","        \"The concert starts soon.\",\n","        \"She painted the room yellow.\",\n","        \"The sky is blue today.\",\n","        \"Birds are singing in the trees.\",\n","        \"The ocean looks calm.\",\n","        \"Mountains surround the valley.\",\n","        \"Children are playing in the park.\",\n","        \"The restaurant serves Italian food.\",\n","        \"Her dress is red and elegant.\",\n","        \"The clock shows three o'clock.\",\n","        \"Music fills the air.\",\n","        \"The garden needs watering.\",\n","        \"His smile is contagious.\",\n","        \"The painting depicts a landscape.\",\n","        \"The library has many books.\",\n","        \"Her voice is melodious.\",\n","        \"The car needs repairs.\",\n","        \"The sunset is beautiful.\",\n","        \"The room is spacious and bright.\",\n","        \"The puppy is adorable.\",\n","        \"The water is crystal clear.\",\n","        \"The cake looks appetizing.\",\n","        \"She walked to the store yesterday.\",\n","        \"They watched a movie last night.\",\n","        \"He cooked dinner for his family.\",\n","        \"The team celebrated their victory.\",\n","        \"She writes in her journal daily.\",\n","        \"They traveled across Europe.\",\n","        \"He runs five miles every morning.\",\n","        \"The kids built a sandcastle.\",\n","        \"She organized the files carefully.\",\n","        \"They planted flowers in the garden.\",\n","        \"He repaired the broken fence.\",\n","        \"The artist created a masterpiece.\",\n","        \"She baked cookies for the party.\",\n","        \"They explored the ancient ruins.\",\n","        \"He learned to play the piano.\",\n","        \"The children sang songs together.\",\n","        \"She knitted a warm scarf.\",\n","        \"They renovated the old house.\",\n","        \"He photographed the wildlife.\",\n","        \"The dancers performed gracefully.\",\n","        \"The water is cold.\",\n","        \"The fabric feels soft.\",\n","        \"The room smells fresh.\",\n","        \"The music sounds peaceful.\",\n","        \"The surface is smooth.\",\n","        \"The night is dark.\",\n","        \"The air feels humid.\",\n","        \"The bread tastes stale.\",\n","        \"The light is dim.\",\n","        \"The ground is uneven.\",\n","        \"The atmosphere is tense.\",\n","        \"The mood is cheerful.\",\n","        \"The texture is rough.\",\n","        \"The temperature is moderate.\",\n","        \"The pressure is intense.\",\n","        \"The pace is slow.\",\n","        \"The style is modern.\",\n","        \"The tone is friendly.\",\n","        \"The flavor is spicy.\",\n","        \"The color is vibrant.\",\n","        \"I prefer tea over coffee.\",\n","        \"She thinks the movie is overrated.\",\n","        \"He believes in hard work.\",\n","        \"They enjoy outdoor activities.\",\n","        \"She appreciates good art.\",\n","        \"He values honesty.\",\n","        \"They admire courage.\",\n","        \"She finds mathematics interesting.\",\n","        \"He considers himself fortunate.\",\n","        \"They regard it as important.\",\n","        \"She loves classical music.\",\n","        \"He likes spicy food.\",\n","        \"They treasure old photographs.\",\n","        \"She cherishes her memories.\",\n","        \"He respects different opinions.\",\n","        \"They favor sustainable practices.\",\n","        \"She prefers quiet evenings.\",\n","        \"He enjoys intellectual discussions.\",\n","        \"They appreciate fine dining.\",\n","        \"She adores her grandchildren.\",\n","        \"The phone is ringing loudly.\",\n","        \"The traffic is heavy this morning.\",\n","        \"The leaves are changing colors.\",\n","        \"The baby is sleeping peacefully.\",\n","        \"The crowd cheered enthusiastically.\",\n","        \"The engine is making strange noises.\",\n","        \"The project deadline is approaching.\",\n","        \"The students are taking notes.\",\n","        \"The wind is blowing strongly.\",\n","        \"The fireplace is burning brightly.\",\n","        \"The snow is falling gently.\",\n","        \"The audience applauded warmly.\",\n","        \"The bread is baking in the oven.\",\n","        \"The river flows through the city.\",\n","        \"The clock is ticking quietly.\",\n","        \"The fog is rolling in.\",\n","        \"The champagne is chilling.\",\n","        \"The documents are being printed.\",\n","        \"The elevator is descending.\",\n","        \"The candles are flickering softly.\",\n","        \"His handwriting is illegible.\",\n","        \"The software is updating automatically.\",\n","        \"The news is spreading quickly.\",\n","        \"The battery is draining fast.\",\n","        \"The crowd is dispersing gradually.\",\n","        \"The ice is melting slowly.\",\n","        \"The prices are fluctuating daily.\",\n","        \"The negotiations are ongoing.\",\n","        \"The situation is improving steadily.\",\n","        \"The performance exceeded expectations.\",\n","        \"The product received positive reviews.\",\n","        \"The conference attracted many attendees.\",\n","        \"The curriculum includes various subjects.\",\n","        \"The menu offers vegetarian options.\",\n","        \"The neighborhood is quiet and safe.\",\n","        \"The festival features local artists.\",\n","        \"The collection showcases contemporary art.\",\n","        \"The documentary explores historical events.\",\n","        \"The workshop teaches practical skills.\",\n","        \"The campaign raised significant funds.\",\n","    ]\n","\n","    data = []\n","    for sent in logical_sentences:\n","        data.append({'sentence': sent, 'label': 1, 'is_perturbed': False, 'source': 'original'})\n","    for sent in non_logical_sentences:\n","        data.append({'sentence': sent, 'label': 0, 'is_perturbed': False, 'source': 'original'})\n","\n","    print(f\"Created base dataset: {len(logical_sentences)} logical, {len(non_logical_sentences)} non-logical\")\n","    return data, logical_sentences  # Return logical sentences for Experiment 3\n","\n","\n","def augment_with_multi_perturbations(data, generator, rounds=[1, 2, 3], aug_per_round=1.0):\n","    \"\"\"Augment dataset with multiple rounds of perturbations\"\"\"\n","\n","    augmented_data = data.copy()\n","    logical_sentences = [d for d in data if d['label'] == 1]\n","    perturbed_map = defaultdict(list)  # Track perturbations for Experiment 3\n","\n","    print(f\"\\nGenerating adversarial examples with {len(rounds)} perturbation rounds...\")\n","\n","    for num_perturbs in rounds:\n","        num_augmentations = int(len(logical_sentences) * aug_per_round)\n","\n","        for _ in range(num_augmentations):\n","            original = random.choice(logical_sentences)\n","            perturbed_sent = generator.generate_multi_perturbation(\n","                original['sentence'],\n","                num_perturbations=num_perturbs\n","            )\n","\n","            if perturbed_sent != original['sentence']:\n","                augmented_data.append({\n","                    'sentence': perturbed_sent,\n","                    'label': 0,\n","                    'is_perturbed': True,\n","                    'source': f'perturb_{num_perturbs}x'\n","                })\n","                # Track for Experiment 3\n","                perturbed_map[original['sentence']].append(perturbed_sent)\n","\n","        print(f\"  Round {num_perturbs}: Generated {num_augmentations} examples\")\n","\n","    return augmented_data, perturbed_map"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1761184542129,"user":{"displayName":"Mohor Bhattacharya","userId":"06292635116005505970"},"user_tz":-780},"id":"AnCUgbVva1Sj"},"outputs":[],"source":["class HardNegativeMiner:\n","    \"\"\"Mine hard negative examples that the model gets wrong\"\"\"\n","\n","    def __init__(self, model, tokenizer, device):\n","        self.model = model\n","        self.tokenizer = tokenizer\n","        self.device = device\n","        self.hard_negatives = []\n","\n","    def find_hard_negatives(self, data, threshold=0.3):\n","        self.model.eval()\n","        hard_examples = []\n","\n","        with torch.no_grad():\n","            for item in tqdm(data, desc=\"Mining hard negatives\"):\n","                inputs = self.tokenizer(\n","                    item['sentence'],\n","                    return_tensors='pt',\n","                    padding=True,\n","                    truncation=True,\n","                    max_length=128\n","                ).to(self.device)\n","\n","                outputs = self.model(**inputs)\n","                probs = torch.softmax(outputs.logits, dim=1)\n","                pred = torch.argmax(probs, dim=1).item()\n","                confidence = probs[0][pred].item()\n","\n","                is_wrong = (pred != item['label'])\n","                is_uncertain = (confidence < 0.7)\n","\n","                if is_wrong or is_uncertain:\n","                    hard_examples.append({\n","                        **item,\n","                        'confidence': confidence,\n","                        'predicted': pred,\n","                        'is_hard': True\n","                    })\n","\n","        print(f\"Found {len(hard_examples)} hard negative examples\")\n","        return hard_examples\n","\n","    def augment_hard_negatives(self, hard_examples, generator, multiplier=2):\n","        augmented = []\n","\n","        for item in hard_examples:\n","            for _ in range(multiplier):\n","                perturbed = generator.generate_multi_perturbation(\n","                    item['sentence'],\n","                    num_perturbations=random.randint(1, 3)\n","                )\n","\n","                if perturbed != item['sentence']:\n","                    augmented.append({\n","                        'sentence': perturbed,\n","                        'label': 0,\n","                        'is_perturbed': True,\n","                        'source': 'hard_negative_aug'\n","                    })\n","\n","        print(f\"Generated {len(augmented)} augmentations from hard negatives\")\n","        return augmented"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1761184542148,"user":{"displayName":"Mohor Bhattacharya","userId":"06292635116005505970"},"user_tz":-780},"id":"MJAKMQb-a1Vk"},"outputs":[],"source":["class EnsembleClassifier:\n","    \"\"\"Combine neural model with heuristic detector\"\"\"\n","\n","    def __init__(self, model, tokenizer, detector, device, neural_weight=0.7):\n","        self.model = model\n","        self.tokenizer = tokenizer\n","        self.detector = detector\n","        self.device = device\n","        self.neural_weight = neural_weight\n","        self.heuristic_weight = 1.0 - neural_weight\n","\n","    def predict(self, sentence):\n","        self.model.eval()\n","\n","        with torch.no_grad():\n","            inputs = self.tokenizer(\n","                sentence,\n","                return_tensors='pt',\n","                padding=True,\n","                truncation=True,\n","                max_length=128\n","            ).to(self.device)\n","\n","            outputs = self.model(**inputs)\n","            neural_probs = torch.softmax(outputs.logits, dim=1)[0]\n","            neural_score = neural_probs[1].item()\n","\n","        heuristic_score = self.detector.compute_logic_score(sentence)\n","\n","        ensemble_score = (self.neural_weight * neural_score +\n","                         self.heuristic_weight * heuristic_score)\n","\n","        prediction = 1 if ensemble_score >= 0.5 else 0\n","\n","        return {\n","            'prediction': prediction,\n","            'ensemble_score': ensemble_score,\n","            'neural_score': neural_score,\n","            'heuristic_score': heuristic_score\n","        }"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1761184542175,"user":{"displayName":"Mohor Bhattacharya","userId":"06292635116005505970"},"user_tz":-780},"id":"Bm1qtV_Fa1Y1"},"outputs":[],"source":["class LogicDataset(Dataset):\n","    \"\"\"PyTorch Dataset for logical sentence classification\"\"\"\n","\n","    def __init__(self, data, tokenizer, max_length=128):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        item = self.data[idx]\n","\n","        encoding = self.tokenizer(\n","            item['sentence'],\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","\n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'label': torch.tensor(item['label'], dtype=torch.long),\n","            'sentence': item['sentence']\n","        }\n","\n","\n","def train_epoch(model, dataloader, optimizer, scheduler, device):\n","    \"\"\"Train for one epoch\"\"\"\n","    model.train()\n","    total_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    for batch in tqdm(dataloader, desc=\"Training\"):\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['label'].to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            labels=labels\n","        )\n","\n","        loss = outputs.loss\n","        logits = outputs.logits\n","\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","        scheduler.step()\n","\n","        total_loss += loss.item()\n","\n","        preds = torch.argmax(logits, dim=1)\n","        correct += (preds == labels).sum().item()\n","        total += labels.size(0)\n","\n","    return total_loss / len(dataloader), correct / total\n","\n","\n","def evaluate(model, dataloader, device):\n","    \"\"\"Evaluate the model\"\"\"\n","    model.eval()\n","    total_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['label'].to(device)\n","\n","            outputs = model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                labels=labels\n","            )\n","\n","            loss = outputs.loss\n","            logits = outputs.logits\n","\n","            total_loss += loss.item()\n","\n","            preds = torch.argmax(logits, dim=1)\n","            correct += (preds == labels).sum().item()\n","            total += labels.size(0)\n","\n","    return total_loss / len(dataloader), correct / total"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":35,"status":"ok","timestamp":1761184542226,"user":{"displayName":"Mohor Bhattacharya","userId":"06292635116005505970"},"user_tz":-780},"id":"zMgRcyOfa1b8"},"outputs":[],"source":["# ============================================================================\n","# EXPERIMENT 3 COMPONENTS (Contrastive Learning)\n","# ============================================================================\n","\n","class LogicalPairConstructor:\n","    \"\"\"Constructs positive and negative logical pairs\"\"\"\n","\n","    def __init__(self):\n","        self.paraphrase_templates = {\n","            'conditional': [\n","                ('if {premise}, {conclusion}', 'when {premise}, {conclusion}'),\n","                ('if {premise}, {conclusion}', '{conclusion} when {premise}'),\n","                ('if {premise}, then {conclusion}', '{conclusion} if {premise}'),\n","            ],\n","            'causal': [\n","                ('because {premise}, {conclusion}', '{conclusion} because {premise}'),\n","                ('since {premise}, {conclusion}', '{conclusion} since {premise}'),\n","                ('{conclusion} because {premise}', '{premise} causes {conclusion}'),\n","                ('because {premise}, {conclusion}', '{premise} leads to {conclusion}'),\n","            ]\n","        }\n","\n","    def extract_premise_conclusion(self, sentence):\n","        \"\"\"Extract premise and conclusion from logical sentence\"\"\"\n","        sentence_lower = sentence.lower()\n","\n","        # Conditional patterns\n","        if_match = re.search(r'if\\s+(.+?),\\s*(.+)', sentence_lower)\n","        if if_match:\n","            return {\n","                'premise': if_match.group(1).strip(),\n","                'conclusion': if_match.group(2).strip(),\n","                'type': 'conditional',\n","                'original': sentence\n","            }\n","\n","        when_match = re.search(r'when\\s+(.+?),\\s*(.+)', sentence_lower)\n","        if when_match:\n","            return {\n","                'premise': when_match.group(1).strip(),\n","                'conclusion': when_match.group(2).strip(),\n","                'type': 'conditional',\n","                'original': sentence\n","            }\n","\n","        # Causal patterns\n","        because_match = re.search(r'because\\s+(.+?),\\s*(.+)', sentence_lower)\n","        if because_match:\n","            return {\n","                'premise': because_match.group(1).strip(),\n","                'conclusion': because_match.group(2).strip(),\n","                'type': 'causal',\n","                'original': sentence\n","            }\n","\n","        since_match = re.search(r'since\\s+(.+?),\\s*(.+)', sentence_lower)\n","        if since_match:\n","            return {\n","                'premise': since_match.group(1).strip(),\n","                'conclusion': since_match.group(2).strip(),\n","                'type': 'causal',\n","                'original': sentence\n","            }\n","\n","        therefore_match = re.search(r'(.+?),\\s*(?:therefore|thus|hence)\\s+(.+)', sentence_lower)\n","        if therefore_match:\n","            return {\n","                'premise': therefore_match.group(1).strip(),\n","                'conclusion': therefore_match.group(2).strip(),\n","                'type': 'causal',\n","                'original': sentence\n","            }\n","\n","        return None\n","\n","    def create_positive_pair(self, sentence):\n","        \"\"\"Create positive pair: original + paraphrase\"\"\"\n","        parsed = self.extract_premise_conclusion(sentence)\n","        if not parsed:\n","            return None\n","\n","        premise = parsed['premise']\n","        conclusion = parsed['conclusion']\n","        logic_type = parsed['type']\n","\n","        if logic_type in self.paraphrase_templates:\n","            templates = self.paraphrase_templates[logic_type]\n","            template_pair = random.choice(templates)\n","\n","            paraphrase = template_pair[1].format(premise=premise, conclusion=conclusion)\n","            paraphrase = paraphrase[0].upper() + paraphrase[1:] + '.'\n","\n","            return {\n","                'sentence1': sentence,\n","                'sentence2': paraphrase,\n","                'label': 1,\n","                'type': 'paraphrase',\n","                'logic_type': logic_type\n","            }\n","\n","        return None\n","\n","    def create_negative_from_perturbation(self, original, perturbed):\n","        \"\"\"Create negative pair from adversarial perturbation\"\"\"\n","        return {\n","            'sentence1': original,\n","            'sentence2': perturbed,\n","            'label': 0,\n","            'type': 'adversarial_negative',\n","            'logic_type': 'broken'\n","        }\n","\n","    def create_random_negative(self, sentence, all_sentences):\n","        \"\"\"Create negative pair: random unrelated sentences\"\"\"\n","        other = random.choice([s for s in all_sentences if s != sentence])\n","        return {\n","            'sentence1': sentence,\n","            'sentence2': other,\n","            'label': 0,\n","            'type': 'random_negative',\n","            'logic_type': 'unrelated'\n","        }\n","\n","\n","def construct_contrastive_dataset(logical_sentences, perturbed_map):\n","    \"\"\"Construct contrastive dataset with positive and negative pairs\"\"\"\n","\n","    constructor = LogicalPairConstructor()\n","    pairs = []\n","\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"CONSTRUCTING CONTRASTIVE PAIRS FOR EXPERIMENT 3\")\n","    print(\"=\"*80)\n","    # 1. Create positive pairs (paraphrases)\n","    print(\"\\n[1/3] Creating positive pairs (paraphrases)...\")\n","    positive_count = 0\n","    for sentence in tqdm(logical_sentences):\n","        positive_pair = constructor.create_positive_pair(sentence)\n","        if positive_pair:\n","            pairs.append(positive_pair)\n","            positive_count += 1\n","\n","    print(f\"✓ Created {positive_count} positive pairs\")\n","\n","    # 2. Create negative pairs from adversarial perturbations\n","    print(\"\\n[2/3] Creating negative pairs (adversarial perturbations)...\")\n","    adversarial_count = 0\n","    for original, perturbed_list in perturbed_map.items():\n","        for perturbed in perturbed_list[:2]:  # Top 2 perturbations per sentence\n","            neg_pair = constructor.create_negative_from_perturbation(original, perturbed)\n","            pairs.append(neg_pair)\n","            adversarial_count += 1\n","\n","    print(f\"✓ Created {adversarial_count} adversarial negative pairs\")\n","\n","    # 3. Create random negative pairs\n","    print(\"\\n[3/3] Creating negative pairs (random unrelated)...\")\n","    random_count = 0\n","    for sentence in logical_sentences:\n","        for _ in range(2):  # 2 random negatives per sentence\n","            random_neg = constructor.create_random_negative(sentence, logical_sentences)\n","            pairs.append(random_neg)\n","            random_count += 1\n","\n","    print(f\"✓ Created {random_count} random negative pairs\")\n","\n","    # Shuffle\n","    random.shuffle(pairs)\n","\n","    # Statistics\n","    positive_total = sum(1 for p in pairs if p['label'] == 1)\n","    negative_total = sum(1 for p in pairs if p['label'] == 0)\n","\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"Contrastive Dataset Statistics:\")\n","    print(f\"  Total pairs: {len(pairs)}\")\n","    print(f\"  Positive pairs: {positive_total}\")\n","    print(f\"  Negative pairs: {negative_total}\")\n","    print(f\"  Balance: {positive_total/len(pairs):.2%} positive\")\n","    print(\"=\"*80)\n","\n","    return pairs"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1761184542242,"user":{"displayName":"Mohor Bhattacharya","userId":"06292635116005505970"},"user_tz":-780},"id":"BaQWiXQCcoGw"},"outputs":[],"source":["class ContrastiveWrapper(nn.Module):\n","    \"\"\"Wraps Experiment 2 classifier for contrastive learning\"\"\"\n","\n","    def __init__(self, phase2_model, temperature=0.05, freeze_base=False):\n","        super().__init__()\n","\n","        self.encoder = phase2_model.distilbert\n","        self.classifier_head = phase2_model.classifier\n","        self.temperature = temperature\n","\n","        if freeze_base:\n","            for param in self.encoder.parameters():\n","                param.requires_grad = False\n","            print(\"✓ Base encoder frozen\")\n","        else:\n","            print(\"✓ Base encoder unfrozen (full fine-tuning)\")\n","\n","    def mean_pooling(self, token_embeddings, attention_mask):\n","        \"\"\"Mean pooling with attention mask\"\"\"\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n","        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","        return sum_embeddings / sum_mask\n","\n","    def encode(self, input_ids, attention_mask):\n","        \"\"\"Encode sentence to embedding\"\"\"\n","        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n","        embeddings = self.mean_pooling(outputs.last_hidden_state, attention_mask)\n","        embeddings = F.normalize(embeddings, p=2, dim=1)\n","        return embeddings\n","\n","    def forward(self, sent1_ids, sent1_mask, sent2_ids, sent2_mask):\n","        \"\"\"Forward pass for contrastive learning\"\"\"\n","        emb1 = self.encode(sent1_ids, sent1_mask)\n","        emb2 = self.encode(sent2_ids, sent2_mask)\n","        return emb1, emb2\n","\n","    def compute_similarity(self, emb1, emb2):\n","        \"\"\"Compute cosine similarity\"\"\"\n","        return F.cosine_similarity(emb1, emb2)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1761184542246,"user":{"displayName":"Mohor Bhattacharya","userId":"06292635116005505970"},"user_tz":-780},"id":"YGuaDhzNcoKf"},"outputs":[],"source":["class InfoNCELoss(nn.Module):\n","    \"\"\"InfoNCE Contrastive Loss\"\"\"\n","\n","    def __init__(self, temperature=0.05):\n","        super().__init__()\n","        self.temperature = temperature\n","\n","    def forward(self, emb1, emb2, labels):\n","        batch_size = emb1.size(0)\n","\n","        similarity_matrix = torch.matmul(emb1, emb2.T) / self.temperature\n","\n","        labels = labels.float()\n","\n","        loss = 0\n","        for i in range(batch_size):\n","            if labels[i] == 1:  # Positive pair\n","                pos_sim = similarity_matrix[i, i]\n","                all_sims = torch.exp(similarity_matrix[i])\n","                loss += -torch.log(torch.exp(pos_sim) / all_sims.sum())\n","            else:  # Negative pair\n","                neg_sim = similarity_matrix[i, i]\n","                loss += torch.exp(neg_sim)\n","\n","        return loss / batch_size"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1761184542265,"user":{"displayName":"Mohor Bhattacharya","userId":"06292635116005505970"},"user_tz":-780},"id":"amXfHPOXdFOa"},"outputs":[],"source":["class ContrastiveHardNegativeMiner:\n","    \"\"\"Mine hard negative pairs using Phase 2 model uncertainty\"\"\"\n","\n","    def __init__(self, phase2_model, tokenizer, device):\n","        self.phase2_model = phase2_model\n","        self.tokenizer = tokenizer\n","        self.device = device\n","\n","    def get_uncertainty_score(self, sentence):\n","        \"\"\"Get Phase 2 model's uncertainty\"\"\"\n","        self.phase2_model.eval()\n","\n","        inputs = self.tokenizer(\n","            sentence,\n","            return_tensors='pt',\n","            padding=True,\n","            truncation=True,\n","            max_length=128\n","        ).to(self.device)\n","\n","        with torch.no_grad():\n","            outputs = self.phase2_model(**inputs)\n","            probs = torch.softmax(outputs.logits, dim=1)\n","            confidence = probs.max().item()\n","\n","        return 1.0 - confidence\n","\n","    def mine_hard_negatives(self, pairs, top_k=50):\n","        \"\"\"Mine hard negative pairs\"\"\"\n","        print(\"\\n[Hard Negative Mining] Finding uncertain pairs...\")\n","\n","        hard_negatives = []\n","\n","        for pair in tqdm(pairs):\n","            if pair['label'] == 0:  # Only negative pairs\n","                unc1 = self.get_uncertainty_score(pair['sentence1'])\n","                unc2 = self.get_uncertainty_score(pair['sentence2'])\n","\n","                avg_uncertainty = (unc1 + unc2) / 2\n","\n","                pair['uncertainty'] = avg_uncertainty\n","                hard_negatives.append(pair)\n","\n","        hard_negatives.sort(key=lambda x: x['uncertainty'], reverse=True)\n","\n","        print(f\"✓ Found {len(hard_negatives)} candidate hard negatives\")\n","        print(f\"✓ Selecting top {top_k} most uncertain pairs\")\n","\n","        return hard_negatives[:top_k]"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1761184542284,"user":{"displayName":"Mohor Bhattacharya","userId":"06292635116005505970"},"user_tz":-780},"id":"-1p1ytqhdFQ-"},"outputs":[],"source":["class ContrastivePairDataset(Dataset):\n","    \"\"\"Dataset for contrastive sentence pairs\"\"\"\n","\n","    def __init__(self, pairs, tokenizer, max_length=128):\n","        self.pairs = pairs\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.pairs)\n","\n","    def __getitem__(self, idx):\n","        pair = self.pairs[idx]\n","\n","        sent1_enc = self.tokenizer(\n","            pair['sentence1'],\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","\n","        sent2_enc = self.tokenizer(\n","            pair['sentence2'],\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","\n","        return {\n","            'sent1_ids': sent1_enc['input_ids'].flatten(),\n","            'sent1_mask': sent1_enc['attention_mask'].flatten(),\n","            'sent2_ids': sent2_enc['input_ids'].flatten(),\n","            'sent2_mask': sent2_enc['attention_mask'].flatten(),\n","            'label': torch.tensor(pair['label'], dtype=torch.long)\n","        }\n","\n","\n","def train_contrastive_epoch(model, dataloader, optimizer, scheduler, loss_fn, device):\n","    \"\"\"Train one epoch of contrastive learning\"\"\"\n","    model.train()\n","    total_loss = 0\n","\n","    for batch in tqdm(dataloader, desc=\"Contrastive Training\"):\n","        sent1_ids = batch['sent1_ids'].to(device)\n","        sent1_mask = batch['sent1_mask'].to(device)\n","        sent2_ids = batch['sent2_ids'].to(device)\n","        sent2_mask = batch['sent2_mask'].to(device)\n","        labels = batch['label'].to(device)\n","\n","        optimizer.zero_grad()\n","\n","        emb1, emb2 = model(sent1_ids, sent1_mask, sent2_ids, sent2_mask)\n","        loss = loss_fn(emb1, emb2, labels)\n","\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","        scheduler.step()\n","\n","        total_loss += loss.item()\n","\n","    return total_loss / len(dataloader)\n","\n","\n","def evaluate_contrastive(model, dataloader, device):\n","    \"\"\"Evaluate contrastive model\"\"\"\n","    model.eval()\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n","            sent1_ids = batch['sent1_ids'].to(device)\n","            sent1_mask = batch['sent1_mask'].to(device)\n","            sent2_ids = batch['sent2_ids'].to(device)\n","            sent2_mask = batch['sent2_mask'].to(device)\n","            labels = batch['label'].to(device)\n","\n","            emb1, emb2 = model(sent1_ids, sent1_mask, sent2_ids, sent2_mask)\n","            similarity = model.compute_similarity(emb1, emb2)\n","\n","            predictions = (similarity > 0.5).long()\n","\n","            correct += (predictions == labels).sum().item()\n","            total += labels.size(0)\n","\n","    return correct / total if total > 0 else 0"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1761184542305,"user":{"displayName":"Mohor Bhattacharya","userId":"06292635116005505970"},"user_tz":-780},"id":"JTk1nUTBdFTr"},"outputs":[],"source":["# ============================================================================\n","# MAIN INTEGRATED PIPELINE\n","# ============================================================================\n","\n","def run_experiment_2(device):\n","    \"\"\"Run Experiment 2: Adversarial Classifier\"\"\"\n","\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"EXPERIMENT 2: ADVERSARIAL CLASSIFIER WITH HARD NEGATIVE MINING\")\n","    print(\"=\"*80)\n","\n","    # Configuration\n","    batch_size = 16\n","    num_epochs = 8\n","    learning_rate = 2e-5\n","\n","    # Initialize\n","    print(\"\\n[1/6] Initializing components...\")\n","    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","    model = DistilBertForSequenceClassification.from_pretrained(\n","        'distilbert-base-uncased',\n","        num_labels=2\n","    ).to(device)\n","\n","    detector = LogicalSentenceDetector()\n","    generator = AdversarialGenerator()\n","\n","    # Create dataset\n","    print(\"\\n[2/6] Creating expanded dataset...\")\n","    initial_data, logical_sentences = create_expanded_dataset()\n","\n","    # Augment\n","    print(\"\\n[3/6] Generating adversarial examples...\")\n","    augmented_data, perturbed_map = augment_with_multi_perturbations(\n","        initial_data,\n","        generator,\n","        rounds=[1, 2, 3],\n","        aug_per_round=0.8\n","    )\n","\n","    # Split\n","    train_data, val_data = train_test_split(augmented_data, test_size=0.2, random_state=42)\n","\n","    train_dataset = LogicDataset(train_data, tokenizer)\n","    val_dataset = LogicDataset(val_data, tokenizer)\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","    # Optimizer\n","    optimizer = AdamW(model.parameters(), lr=learning_rate)\n","    total_steps = len(train_loader) * num_epochs\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=int(0.1 * total_steps),\n","        num_training_steps=total_steps\n","    )\n","\n","    # Train\n","    print(\"\\n[4/6] Training with hard negative mining...\")\n","    best_val_acc = 0\n","    hard_negative_miner = HardNegativeMiner(model, tokenizer, device)\n","\n","    for epoch in range(num_epochs):\n","        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n","        print(\"-\" * 80)\n","\n","        train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, device)\n","        val_loss, val_acc = evaluate(model, val_loader, device)\n","\n","        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n","        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n","\n","        if val_acc > best_val_acc:\n","            best_val_acc = val_acc\n","            torch.save(model.state_dict(), 'best_logic_classifier.pt')\n","            print(f\"✓ Best model saved!\")\n","\n","        # Hard negative mining every 2 epochs\n","        if (epoch + 1) % 2 == 0 and epoch < num_epochs - 1:\n","            print(\"\\n  >> Running hard negative mining...\")\n","            hard_examples = hard_negative_miner.find_hard_negatives(train_data, threshold=0.3)\n","\n","            if len(hard_examples) > 10:\n","                hard_augmented = hard_negative_miner.augment_hard_negatives(\n","                    hard_examples, generator, multiplier=2\n","                )\n","\n","                train_data.extend(hard_augmented)\n","                train_dataset = LogicDataset(train_data, tokenizer)\n","                train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","                print(f\"  ✓ Added {len(hard_augmented)} hard negative augmentations\")\n","\n","    print(f\"\\n{'='*80}\")\n","    print(f\"EXPERIMENT 2 COMPLETE!\")\n","    print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n","    print(f\"Model saved: best_logic_classifier.pt\")\n","    print(f\"{'='*80}\")\n","    # Save data for Experiment 3\n","    with open('logical_sentences.json', 'w') as f:\n","        json.dump(logical_sentences, f, indent=2)\n","\n","    with open('perturbed_sentences.json', 'w') as f:\n","        json.dump(perturbed_map, f, indent=2)\n","\n","    print(\"\\n✓ Saved logical_sentences.json and perturbed_sentences.json for Experiment 3\")\n","\n","    return model, tokenizer, logical_sentences, perturbed_map, best_val_acc\n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1761184542310,"user":{"displayName":"Mohor Bhattacharya","userId":"06292635116005505970"},"user_tz":-780},"id":"SbuINdEna1ew"},"outputs":[],"source":["def run_experiment_3(phase2_model, tokenizer, logical_sentences, perturbed_map, device):\n","    \"\"\"Run Experiment 3: Contrastive Learning\"\"\"\n","\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"EXPERIMENT 3: CONTRASTIVE LOGICAL PRETRAINING\")\n","    print(\"=\"*80)\n","\n","    # Configuration\n","    batch_size = 32\n","    num_epochs = 3\n","    learning_rate = 2e-5\n","    use_hard_mining = True\n","\n","    # Construct contrastive pairs\n","    pairs = construct_contrastive_dataset(logical_sentences, perturbed_map)\n","\n","    # Hard negative mining\n","    if use_hard_mining:\n","        print(\"\\n[Optional] Mining hard negatives from Phase 2 uncertainty...\")\n","        miner = ContrastiveHardNegativeMiner(phase2_model, tokenizer, device)\n","        hard_negatives = miner.mine_hard_negatives(pairs, top_k=50)\n","\n","        pairs.extend(hard_negatives * 2)  # 2x weight\n","        random.shuffle(pairs)\n","        print(f\"✓ Dataset now contains {len(pairs)} pairs (including hard negatives)\")\n","\n","    # Split\n","    train_pairs, val_pairs = train_test_split(pairs, test_size=0.2, random_state=42)\n","    print(f\"\\n✓ Split: {len(train_pairs)} train, {len(val_pairs)} validation\")\n","\n","    # Dataloaders\n","    train_dataset = ContrastivePairDataset(train_pairs, tokenizer)\n","    val_dataset = ContrastivePairDataset(val_pairs, tokenizer)\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","    # Create contrastive model\n","    print(\"\\n[Model Setup] Wrapping Phase 2 model with contrastive objective...\")\n","    contrastive_model = ContrastiveWrapper(\n","        phase2_model,\n","        temperature=0.05,\n","        freeze_base=False\n","    ).to(device)\n","\n","    # Optimizer\n","    loss_fn = InfoNCELoss(temperature=0.05)\n","    optimizer = AdamW(contrastive_model.parameters(), lr=learning_rate)\n","    total_steps = len(train_loader) * num_epochs\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=int(0.1 * total_steps),\n","        num_training_steps=total_steps\n","    )\n","\n","    # Train\n","    print(\"\\n[Training] Lightweight contrastive training (2-3 epochs)...\")\n","    best_val_acc = 0\n","\n","    for epoch in range(num_epochs):\n","        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n","        print(\"-\" * 80)\n","\n","        train_loss = train_contrastive_epoch(\n","            contrastive_model, train_loader, optimizer, scheduler, loss_fn, device\n","        )\n","        val_acc = evaluate_contrastive(contrastive_model, val_loader, device)\n","\n","        print(f\"Train Loss: {train_loss:.4f} | Val Accuracy: {val_acc:.4f}\")\n","\n","        if val_acc > best_val_acc:\n","            best_val_acc = val_acc\n","            torch.save(contrastive_model.state_dict(), 'contrastive_model_exp3.pt')\n","            print(f\"✓ Best contrastive model saved!\")\n","\n","    print(f\"\\n{'='*80}\")\n","    print(f\"EXPERIMENT 3 COMPLETE!\")\n","    print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n","    print(f\"Model saved: contrastive_model_exp3.pt\")\n","    print(f\"{'='*80}\")\n","\n","    # Test\n","    print(\"\\n[Testing] Sample predictions...\")\n","    print(\"-\" * 80)\n","\n","    contrastive_model.eval()\n","    test_pairs = [\n","        (\"If it rains, the ground will be wet.\", \"When it rains, the ground becomes wet.\", True),\n","        (\"Because he studied, he passed.\", \"His studying led to passing the exam.\", True),\n","        (\"If it rains, the ground will be wet.\", \"The cat is sleeping.\", False),\n","        (\"Because he studied, he passed.\", \"The sky is blue.\", False),\n","    ]\n","\n","    with torch.no_grad():\n","        for sent1, sent2, is_related in test_pairs:\n","            enc1 = tokenizer(sent1, return_tensors='pt', padding=True, truncation=True).to(device)\n","            enc2 = tokenizer(sent2, return_tensors='pt', padding=True, truncation=True).to(device)\n","\n","            emb1 = contrastive_model.encode(enc1['input_ids'], enc1['attention_mask'])\n","            emb2 = contrastive_model.encode(enc2['input_ids'], enc2['attention_mask'])\n","\n","            similarity = contrastive_model.compute_similarity(emb1, emb2).item()\n","            relation = \"RELATED\" if is_related else \"UNRELATED\"\n","\n","            print(f\"\\n[{relation}]\")\n","            print(f\"  Sent 1: '{sent1}'\")\n","            print(f\"  Sent 2: '{sent2}'\")\n","            print(f\"  → Similarity: {similarity:.4f}\")\n","\n","    # Save results\n","    results = {\n","        'experiment': 'contrastive_extension',\n","        'num_epochs': num_epochs,\n","        'best_val_accuracy': best_val_acc,\n","        'dataset_size': len(pairs)\n","    }\n","\n","    with open('experiment3_results.json', 'w') as f:\n","        json.dump(results, f, indent=2)\n","\n","    print(\"\\n✓ Results saved to: experiment3_results.json\")\n","\n","    return contrastive_model, best_val_acc"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["b23ff0348c004b3aaf891e05373384f8","649a90c9262d41c7a84af766399e351b","d6565cae2f3a459b976c4fa3fea40fd4","9c500c89f0484b44a1dd614b2765dce2","92574df6d2c84e52a655819e9cd10e44","f9986c46f5cf40109c0f478d51379bb0","7df3df9717c64a3e921c601187d8fd9f","d522c5c5563f4b76a1d0ec8b984dd463","d6409b5ff7bf4a0ead9f0ee3f9d1f48d","0792c8f457ae46a0a5ee6cc280caba7a","61f0637441924282b81834f7a41269ec","249b3e86599f426a9f53664cb4349579","1e0773aa2836488c8b30f812f796a8c6","f57f2d892b83430eac5474373c13ed35","a76c182fa22448b98ae229d2fe56ef07","32b02f8ee3f54f36932ca3de1ffb24f0","a79c261d443f4471b6942dceacb587e6","6f1d762a2c244a99b33f09f376c3e5a9","338307508d724968b26ea1ea876488c3","23ecfb2aa91f40f19bc56441f82e03c6","7c6f3fda93ac4440aff6a3c796945e82","7c1eb590371449388260033e6471a60d","c3320b5b918d46d8aec296e37220e39e","8ab9a77f214c49b191bb86a4299fc2ea","14c0eca48604443dba808dd237ea8e3a","a3a44858e80448c4806ed5e11884d5eb","f48fb9b4aa0a464983f5257bbfa7968c","bd3700c281d3434f89295461b25f7dea","4a74e63062e64ea7860962369a8c6bb0","7c69be79341f49cb976a3829aef981a0","148a7d17fed64cbdb0d8c39fd93be216","8ea46fbef87c42cd9b777022a19bdaf5","1ec5d76a1bf84ac2a505b1a1342dde4f","8410c63b014044339d8c71a90b63728b","6ffa4f16b2464b848b01670a4e8f363e","fd77544a78a04c4692efd6ed605cf3b8","7f79525d875a4ac49306caefea1f0baf","a0e1b29d3da7439495d6f579200d00a8","426b88e4ad3349ae90338cea4c08cfb0","6c6993154304418d991e3ff5fcc21ee0","8422d74b928b4a8ba352da1f8505a2fe","b969c961ba3a40de808b1a0068de5546","b375346993e542b7a4a7e39fbc590ab2","9daea2215935440296b081b86f6f2340","36fc8039718d4e419ae14cc61a424ab6","369bd3ecd13e4393b7bb7aa13efdaa0e","5ad1f4dde0e244afb056dfa798a4d956","38b3c4dc080d4190b65145e9596541a1","472addcc5da74c8eaca834e24aeefaa9","9ca567b98478422785128fb0a99edf81","482fc8dadf5f411e97701c22b128d3fa","1b6cd333867f4f6eb9c76bd5283560db","3167b9cdc1b14a6d9f8bc7e95fd9d960","96001ac87c264526979233e07467aeb3","6b49e999f6f747db8b490f86f31c1fc0"]},"id":"q4p06QNKa1hs","outputId":"07d5b7e8-f75c-48ee-dc47-a5966afd44b3","executionInfo":{"status":"ok","timestamp":1761184670812,"user_tz":-780,"elapsed":128499,"user":{"displayName":"Mohor Bhattacharya","userId":"06292635116005505970"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","INTEGRATED PIPELINE: EXPERIMENTS 2 → 3\n","================================================================================\n","Device: cuda\n","Running both experiments sequentially...\n","\n","================================================================================\n","EXPERIMENT 2: ADVERSARIAL CLASSIFIER WITH HARD NEGATIVE MINING\n","================================================================================\n","\n","[1/6] Initializing components...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b23ff0348c004b3aaf891e05373384f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"249b3e86599f426a9f53664cb4349579"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3320b5b918d46d8aec296e37220e39e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8410c63b014044339d8c71a90b63728b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36fc8039718d4e419ae14cc61a424ab6"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["\n","[2/6] Creating expanded dataset...\n","Created base dataset: 130 logical, 140 non-logical\n","\n","[3/6] Generating adversarial examples...\n","\n","Generating adversarial examples with 3 perturbation rounds...\n","  Round 1: Generated 104 examples\n","  Round 2: Generated 104 examples\n","  Round 3: Generated 104 examples\n","\n","[4/6] Training with hard negative mining...\n","\n","Epoch 1/8\n","--------------------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 28/28 [00:05<00:00,  4.78it/s]\n","Evaluating: 100%|██████████| 7/7 [00:00<00:00, 17.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.6091 | Train Acc: 0.7383\n","Val Loss: 0.5787 | Val Acc: 0.7143\n","✓ Best model saved!\n","\n","Epoch 2/8\n","--------------------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 28/28 [00:04<00:00,  5.88it/s]\n","Evaluating: 100%|██████████| 7/7 [00:00<00:00, 18.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.3999 | Train Acc: 0.7808\n","Val Loss: 0.3433 | Val Acc: 0.7143\n","\n","  >> Running hard negative mining...\n"]},{"output_type":"stream","name":"stderr","text":["Mining hard negatives: 100%|██████████| 447/447 [00:01<00:00, 238.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Found 132 hard negative examples\n","Generated 238 augmentations from hard negatives\n","  ✓ Added 238 hard negative augmentations\n","\n","Epoch 3/8\n","--------------------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 43/43 [00:07<00:00,  5.78it/s]\n","Evaluating: 100%|██████████| 7/7 [00:00<00:00, 17.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.1672 | Train Acc: 0.9299\n","Val Loss: 0.1552 | Val Acc: 0.9375\n","✓ Best model saved!\n","\n","Epoch 4/8\n","--------------------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 43/43 [00:07<00:00,  5.71it/s]\n","Evaluating: 100%|██████████| 7/7 [00:00<00:00, 16.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.0946 | Train Acc: 0.9693\n","Val Loss: 0.1107 | Val Acc: 0.9643\n","✓ Best model saved!\n","\n","  >> Running hard negative mining...\n"]},{"output_type":"stream","name":"stderr","text":["Mining hard negatives: 100%|██████████| 685/685 [00:02<00:00, 239.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Found 11 hard negative examples\n","Generated 17 augmentations from hard negatives\n","  ✓ Added 17 hard negative augmentations\n","\n","Epoch 5/8\n","--------------------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 44/44 [00:08<00:00,  4.94it/s]\n","Evaluating: 100%|██████████| 7/7 [00:00<00:00,  8.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.0692 | Train Acc: 0.9801\n","Val Loss: 0.0984 | Val Acc: 0.9732\n","✓ Best model saved!\n","\n","Epoch 6/8\n","--------------------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 44/44 [00:08<00:00,  5.12it/s]\n","Evaluating: 100%|██████████| 7/7 [00:00<00:00, 16.40it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.0525 | Train Acc: 0.9858\n","Val Loss: 0.0848 | Val Acc: 0.9732\n","\n","  >> Running hard negative mining...\n"]},{"output_type":"stream","name":"stderr","text":["Mining hard negatives: 100%|██████████| 702/702 [00:03<00:00, 217.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Found 10 hard negative examples\n","\n","Epoch 7/8\n","--------------------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 44/44 [00:08<00:00,  5.44it/s]\n","Evaluating: 100%|██████████| 7/7 [00:00<00:00, 16.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.0425 | Train Acc: 0.9858\n","Val Loss: 0.0848 | Val Acc: 0.9732\n","\n","Epoch 8/8\n","--------------------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 44/44 [00:08<00:00,  5.49it/s]\n","Evaluating: 100%|██████████| 7/7 [00:00<00:00, 17.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.0519 | Train Acc: 0.9858\n","Val Loss: 0.0848 | Val Acc: 0.9732\n","\n","================================================================================\n","EXPERIMENT 2 COMPLETE!\n","Best validation accuracy: 0.9732\n","Model saved: best_logic_classifier.pt\n","================================================================================\n","\n","✓ Saved logical_sentences.json and perturbed_sentences.json for Experiment 3\n","\n","================================================================================\n","EXPERIMENT 3: CONTRASTIVE LOGICAL PRETRAINING\n","================================================================================\n","\n","================================================================================\n","CONSTRUCTING CONTRASTIVE PAIRS FOR EXPERIMENT 3\n","================================================================================\n","\n","[1/3] Creating positive pairs (paraphrases)...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:00<00:00, 28998.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["✓ Created 68 positive pairs\n","\n","[2/3] Creating negative pairs (adversarial perturbations)...\n","✓ Created 203 adversarial negative pairs\n","\n","[3/3] Creating negative pairs (random unrelated)...\n","✓ Created 260 random negative pairs\n","\n","================================================================================\n","Contrastive Dataset Statistics:\n","  Total pairs: 531\n","  Positive pairs: 68\n","  Negative pairs: 463\n","  Balance: 12.81% positive\n","================================================================================\n","\n","[Optional] Mining hard negatives from Phase 2 uncertainty...\n","\n","[Hard Negative Mining] Finding uncertain pairs...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 531/531 [00:04<00:00, 114.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["✓ Found 463 candidate hard negatives\n","✓ Selecting top 50 most uncertain pairs\n","✓ Dataset now contains 631 pairs (including hard negatives)\n","\n","✓ Split: 504 train, 127 validation\n","\n","[Model Setup] Wrapping Phase 2 model with contrastive objective...\n","✓ Base encoder unfrozen (full fine-tuning)\n","\n","[Training] Lightweight contrastive training (2-3 epochs)...\n","\n","Epoch 1/3\n","--------------------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Contrastive Training: 100%|██████████| 16/16 [00:10<00:00,  1.58it/s]\n","Evaluating: 100%|██████████| 4/4 [00:00<00:00,  4.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 3678221.3276 | Val Accuracy: 0.5906\n","✓ Best contrastive model saved!\n","\n","Epoch 2/3\n","--------------------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Contrastive Training: 100%|██████████| 16/16 [00:09<00:00,  1.62it/s]\n","Evaluating: 100%|██████████| 4/4 [00:00<00:00,  4.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 67071.6725 | Val Accuracy: 0.8189\n","✓ Best contrastive model saved!\n","\n","Epoch 3/3\n","--------------------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Contrastive Training: 100%|██████████| 16/16 [00:09<00:00,  1.61it/s]\n","Evaluating: 100%|██████████| 4/4 [00:00<00:00,  4.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 25222.8713 | Val Accuracy: 0.8346\n","✓ Best contrastive model saved!\n","\n","================================================================================\n","EXPERIMENT 3 COMPLETE!\n","Best validation accuracy: 0.8346\n","Model saved: contrastive_model_exp3.pt\n","================================================================================\n","\n","[Testing] Sample predictions...\n","--------------------------------------------------------------------------------\n","\n","[RELATED]\n","  Sent 1: 'If it rains, the ground will be wet.'\n","  Sent 2: 'When it rains, the ground becomes wet.'\n","  → Similarity: 0.8437\n","\n","[RELATED]\n","  Sent 1: 'Because he studied, he passed.'\n","  Sent 2: 'His studying led to passing the exam.'\n","  → Similarity: 0.3112\n","\n","[UNRELATED]\n","  Sent 1: 'If it rains, the ground will be wet.'\n","  Sent 2: 'The cat is sleeping.'\n","  → Similarity: -0.1324\n","\n","[UNRELATED]\n","  Sent 1: 'Because he studied, he passed.'\n","  Sent 2: 'The sky is blue.'\n","  → Similarity: 0.0126\n","\n","✓ Results saved to: experiment3_results.json\n","\n","================================================================================\n","PIPELINE COMPLETE - FINAL SUMMARY\n","================================================================================\n","\n","Experiment 2 (Adversarial):\n","  Best Val Accuracy: 0.9732\n","  Model: best_logic_classifier.pt\n","\n","Experiment 3 (Contrastive):\n","  Best Val Accuracy: 0.8346\n","  Model: contrastive_model_exp3.pt\n","\n","✓ All files saved successfully!\n","================================================================================\n"]}],"source":["def main():\n","    \"\"\"Main pipeline: Run Experiments 2 and 3\"\"\"\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    print(\"=\"*80)\n","    print(\"INTEGRATED PIPELINE: EXPERIMENTS 2 → 3\")\n","    print(\"=\"*80)\n","    print(f\"Device: {device}\")\n","    print(f\"Running both experiments sequentially...\")\n","\n","    # Run Experiment 2\n","    phase2_model, tokenizer, logical_sentences, perturbed_map, exp2_acc = run_experiment_2(device)\n","\n","    # Run Experiment 3\n","    contrastive_model, exp3_acc = run_experiment_3(\n","        phase2_model, tokenizer, logical_sentences, perturbed_map, device\n","    )\n","\n","    # Final summary\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"PIPELINE COMPLETE - FINAL SUMMARY\")\n","    print(\"=\"*80)\n","    print(f\"\\nExperiment 2 (Adversarial):\")\n","    print(f\"  Best Val Accuracy: {exp2_acc:.4f}\")\n","    print(f\"  Model: best_logic_classifier.pt\")\n","\n","    print(f\"\\nExperiment 3 (Contrastive):\")\n","    print(f\"  Best Val Accuracy: {exp3_acc:.4f}\")\n","    print(f\"  Model: contrastive_model_exp3.pt\")\n","\n","    print(f\"\\n✓ All files saved successfully!\")\n","    print(\"=\"*80)\n","\n","    return phase2_model, contrastive_model\n","\n","\n","if __name__ == \"__main__\":\n","    phase2_model, contrastive_model = main()"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"z5IB3m9YdKqN","executionInfo":{"status":"ok","timestamp":1761184670827,"user_tz":-780,"elapsed":9,"user":{"displayName":"Mohor Bhattacharya","userId":"06292635116005505970"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":15,"metadata":{"id":"S6bQ9IQ3a1ke","executionInfo":{"status":"ok","timestamp":1761184670855,"user_tz":-780,"elapsed":24,"user":{"displayName":"Mohor Bhattacharya","userId":"06292635116005505970"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":15,"metadata":{"id":"BALn81Q3a1nQ","executionInfo":{"status":"ok","timestamp":1761184670864,"user_tz":-780,"elapsed":3,"user":{"displayName":"Mohor Bhattacharya","userId":"06292635116005505970"}}},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1RebupCEnD0cuI_4v1NKX46ipVx10TawD","timestamp":1761184716575}],"gpuType":"T4","authorship_tag":"ABX9TyOpbTUVmb2iozA1O5u4Ogk2"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b23ff0348c004b3aaf891e05373384f8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_649a90c9262d41c7a84af766399e351b","IPY_MODEL_d6565cae2f3a459b976c4fa3fea40fd4","IPY_MODEL_9c500c89f0484b44a1dd614b2765dce2"],"layout":"IPY_MODEL_92574df6d2c84e52a655819e9cd10e44"}},"649a90c9262d41c7a84af766399e351b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9986c46f5cf40109c0f478d51379bb0","placeholder":"​","style":"IPY_MODEL_7df3df9717c64a3e921c601187d8fd9f","value":"tokenizer_config.json: 100%"}},"d6565cae2f3a459b976c4fa3fea40fd4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d522c5c5563f4b76a1d0ec8b984dd463","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d6409b5ff7bf4a0ead9f0ee3f9d1f48d","value":48}},"9c500c89f0484b44a1dd614b2765dce2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0792c8f457ae46a0a5ee6cc280caba7a","placeholder":"​","style":"IPY_MODEL_61f0637441924282b81834f7a41269ec","value":" 48.0/48.0 [00:00&lt;00:00, 1.00kB/s]"}},"92574df6d2c84e52a655819e9cd10e44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9986c46f5cf40109c0f478d51379bb0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7df3df9717c64a3e921c601187d8fd9f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d522c5c5563f4b76a1d0ec8b984dd463":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6409b5ff7bf4a0ead9f0ee3f9d1f48d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0792c8f457ae46a0a5ee6cc280caba7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61f0637441924282b81834f7a41269ec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"249b3e86599f426a9f53664cb4349579":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1e0773aa2836488c8b30f812f796a8c6","IPY_MODEL_f57f2d892b83430eac5474373c13ed35","IPY_MODEL_a76c182fa22448b98ae229d2fe56ef07"],"layout":"IPY_MODEL_32b02f8ee3f54f36932ca3de1ffb24f0"}},"1e0773aa2836488c8b30f812f796a8c6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a79c261d443f4471b6942dceacb587e6","placeholder":"​","style":"IPY_MODEL_6f1d762a2c244a99b33f09f376c3e5a9","value":"vocab.txt: 100%"}},"f57f2d892b83430eac5474373c13ed35":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_338307508d724968b26ea1ea876488c3","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_23ecfb2aa91f40f19bc56441f82e03c6","value":231508}},"a76c182fa22448b98ae229d2fe56ef07":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c6f3fda93ac4440aff6a3c796945e82","placeholder":"​","style":"IPY_MODEL_7c1eb590371449388260033e6471a60d","value":" 232k/232k [00:00&lt;00:00, 3.95MB/s]"}},"32b02f8ee3f54f36932ca3de1ffb24f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a79c261d443f4471b6942dceacb587e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f1d762a2c244a99b33f09f376c3e5a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"338307508d724968b26ea1ea876488c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23ecfb2aa91f40f19bc56441f82e03c6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7c6f3fda93ac4440aff6a3c796945e82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c1eb590371449388260033e6471a60d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c3320b5b918d46d8aec296e37220e39e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8ab9a77f214c49b191bb86a4299fc2ea","IPY_MODEL_14c0eca48604443dba808dd237ea8e3a","IPY_MODEL_a3a44858e80448c4806ed5e11884d5eb"],"layout":"IPY_MODEL_f48fb9b4aa0a464983f5257bbfa7968c"}},"8ab9a77f214c49b191bb86a4299fc2ea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd3700c281d3434f89295461b25f7dea","placeholder":"​","style":"IPY_MODEL_4a74e63062e64ea7860962369a8c6bb0","value":"tokenizer.json: 100%"}},"14c0eca48604443dba808dd237ea8e3a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c69be79341f49cb976a3829aef981a0","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_148a7d17fed64cbdb0d8c39fd93be216","value":466062}},"a3a44858e80448c4806ed5e11884d5eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ea46fbef87c42cd9b777022a19bdaf5","placeholder":"​","style":"IPY_MODEL_1ec5d76a1bf84ac2a505b1a1342dde4f","value":" 466k/466k [00:00&lt;00:00, 978kB/s]"}},"f48fb9b4aa0a464983f5257bbfa7968c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd3700c281d3434f89295461b25f7dea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a74e63062e64ea7860962369a8c6bb0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7c69be79341f49cb976a3829aef981a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"148a7d17fed64cbdb0d8c39fd93be216":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8ea46fbef87c42cd9b777022a19bdaf5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ec5d76a1bf84ac2a505b1a1342dde4f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8410c63b014044339d8c71a90b63728b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6ffa4f16b2464b848b01670a4e8f363e","IPY_MODEL_fd77544a78a04c4692efd6ed605cf3b8","IPY_MODEL_7f79525d875a4ac49306caefea1f0baf"],"layout":"IPY_MODEL_a0e1b29d3da7439495d6f579200d00a8"}},"6ffa4f16b2464b848b01670a4e8f363e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_426b88e4ad3349ae90338cea4c08cfb0","placeholder":"​","style":"IPY_MODEL_6c6993154304418d991e3ff5fcc21ee0","value":"config.json: 100%"}},"fd77544a78a04c4692efd6ed605cf3b8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8422d74b928b4a8ba352da1f8505a2fe","max":483,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b969c961ba3a40de808b1a0068de5546","value":483}},"7f79525d875a4ac49306caefea1f0baf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b375346993e542b7a4a7e39fbc590ab2","placeholder":"​","style":"IPY_MODEL_9daea2215935440296b081b86f6f2340","value":" 483/483 [00:00&lt;00:00, 13.0kB/s]"}},"a0e1b29d3da7439495d6f579200d00a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"426b88e4ad3349ae90338cea4c08cfb0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c6993154304418d991e3ff5fcc21ee0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8422d74b928b4a8ba352da1f8505a2fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b969c961ba3a40de808b1a0068de5546":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b375346993e542b7a4a7e39fbc590ab2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9daea2215935440296b081b86f6f2340":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"36fc8039718d4e419ae14cc61a424ab6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_369bd3ecd13e4393b7bb7aa13efdaa0e","IPY_MODEL_5ad1f4dde0e244afb056dfa798a4d956","IPY_MODEL_38b3c4dc080d4190b65145e9596541a1"],"layout":"IPY_MODEL_472addcc5da74c8eaca834e24aeefaa9"}},"369bd3ecd13e4393b7bb7aa13efdaa0e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ca567b98478422785128fb0a99edf81","placeholder":"​","style":"IPY_MODEL_482fc8dadf5f411e97701c22b128d3fa","value":"model.safetensors: 100%"}},"5ad1f4dde0e244afb056dfa798a4d956":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b6cd333867f4f6eb9c76bd5283560db","max":267954768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3167b9cdc1b14a6d9f8bc7e95fd9d960","value":267954768}},"38b3c4dc080d4190b65145e9596541a1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_96001ac87c264526979233e07467aeb3","placeholder":"​","style":"IPY_MODEL_6b49e999f6f747db8b490f86f31c1fc0","value":" 268M/268M [00:07&lt;00:00, 38.0MB/s]"}},"472addcc5da74c8eaca834e24aeefaa9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ca567b98478422785128fb0a99edf81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"482fc8dadf5f411e97701c22b128d3fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1b6cd333867f4f6eb9c76bd5283560db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3167b9cdc1b14a6d9f8bc7e95fd9d960":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"96001ac87c264526979233e07467aeb3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b49e999f6f747db8b490f86f31c1fc0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}