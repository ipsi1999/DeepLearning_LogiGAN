# DeepLearning_LogiGAN

ğŸ§© Step 1. Understanding What Each Phase Does

Phase 1 â€” Baseline Logical Classifier
	Goal: Train a baseline DistilBERT classifier on logical vs non-logical sentences.
Pipeline:
		1. Load pre-trained distilbert-base-uncased.
		2. Fine-tune it on a dataset of logical statements identified via heuristics (like â€œifâ€, â€œbecauseâ€, etc.).
	â€¢	Train using a standard classification loss (CrossEntropyLoss).
	â€¢	Evaluate using accuracy (and possibly loss) on validation set.
	â€¢	Output: Baseline model accuracy = your reference point.

So Phase 1 = plain supervised fine-tuning.

â¸»

Phase 2 â€” Adversarial Fine-tuning with Hard Negative Mining
	â€¢	Goal: Make the model more robust by exposing it to perturbed logical sentences.
	â€¢	New components:
	â€¢	AdversarialGenerator() â†’ creates perturbed (slightly altered or misleading) versions of logical sentences.
	â€¢	HardNegativeMiner() â†’ identifies â€œdifficultâ€ examples the model is uncertain about, and adds them back into training.
	â€¢	Pipeline:
	1.	Train initial model on dataset (same as Phase 1).
	2.	Use generator to produce adversarial versions.
	3.	Fine-tune the model again on original + adversarial examples.
	4.	Every few epochs, mine â€œhard negativesâ€ and retrain.
	â€¢	Output: Model learns logical robustness and improves generalization.

So Phase 2 = adversarial augmentation + iterative hard-negative mining to increase reasoning consistency.

â¸»

Phase 3 â€” Contrastive Logical Pre-training
	â€¢	Goal: Encourage the model to learn relational similarity between logically equivalent sentences.
	â€¢	New components:
	â€¢	LogicalPairConstructor() â†’ constructs positive and negative sentence pairs:
	â€¢	Positive = logically equivalent (e.g., â€œIf A then Bâ€ â†” â€œWhen A, Bâ€).
	â€¢	Negative = unrelated or adversarial pairs.
	â€¢	ContrastiveWrapper() â†’ wraps the Phase 2 model and outputs sentence embeddings.
	â€¢	InfoNCELoss() â†’ pushes similar pairs together and dissimilar ones apart in embedding space.
	â€¢	Pipeline:
	1.	Build a dataset of sentence pairs (from Phase 2 outputs).
	2.	Train with contrastive loss (instead of cross-entropy).
	3.	Optionally mine hard negatives using uncertainty from Phase 2.
	â€¢	Output: A model that encodes logical relationships more coherently (better semantic alignment).

So Phase 3 = contrastive fine-tuning on logic-consistent sentence pairs.
